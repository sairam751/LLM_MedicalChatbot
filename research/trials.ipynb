{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l:\\\\Pdfs\\\\ReposGitHub\\\\LLM_MedicalChatbot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l:\\\\Pdfs\\\\ReposGitHub\\\\LLM_MedicalChatbot'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\venvmedibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN=os.environ.get('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"llm-medicalbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"llm-medicalbot-x3jcoyp.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 384,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"llm-medicalbot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1d1c3c21a20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='23e76057-047e-4605-a37e-6cccb7efeab9', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='722bc18d-a3b2-4e89-adb3-6e2c3e8dbb74', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed.(Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(id='1fdedc72-029d-4e99-b006-1652d586def7', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 37.0, 'page_label': '38', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'Data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id = \"google/gemma-2b\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\venvmedibot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pituitary gland, located in the center of the brain,\n",
      "secretes a hormone called growth hormone. This hormone\n",
      "stimulates growth of bones and soft tissues throughout\n",
      "the body. Normally, the growth hormone released by the\n",
      "pituitary gland is regulated by the brain. In acromegaly,\n",
      "the pituitary gland produces too much growth hormone,\n",
      "causing the abnormal growth of bones and soft tissue.\n",
      "Symptoms of acromegaly include the following:\n",
      "â€¢ Enlargement of the hands, feet, and face\n",
      "â€¢ Flattened nose\n",
      "â€¢ Thickened lips\n",
      "â€¢ Thickened tongue\n",
      "â€¢ Thickened voice box\n",
      "â€¢ Thickened fingers and toes\n",
      "â€¢ Thickened nails\n",
      "â€¢ Thickened skin\n",
      "â€¢ Increased sweating\n",
      "â€¢ Enlarged tongue\n",
      "â€¢ Enlarged salivary glands\n",
      "â€¢ Enlarged thyroid gland\n",
      "â€¢ Enlarged pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "â€¢ Enlargement of the pituitary gland\n",
      "â€¢ Enlargement of the pituitary stalk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\venvmedibot\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "Human: What is stats?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvmedibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
